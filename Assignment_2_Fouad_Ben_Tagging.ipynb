{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLHeV9icIk-b"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# Natural Language Processing\n",
        "\n",
        "## Assignment 002 - POS Tagging with Universal Dependencies\n",
        "\n",
        "> Notebook by:\n",
        "> - NLP Course Stuff\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "|  |  |    | |\n",
        "| 0.1.000 | 01/05/2024 | Course stuff | First version                                                      |\n",
        "|         |            |             |                                                                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCFoQxO0H0yk"
      },
      "source": [
        "## Overview\n",
        "In this assignment, you will train and evaluate a Part-of-Speech (POS) tagger using data from the Universal Dependencies (UD) project. POS taggers assign parts of speech to each word in a sentence, such as noun, verb, adjective, etc., which are crucial for many natural language processing tasks.\n",
        "\n",
        "## Dataset\n",
        "Utilize the English Web Treebank from the Universal Dependencies project. You can access and explore the dataset [here](https://universaldependencies.org/). For a better understanding of the project and the data format, visit the [introduction page](https://universaldependencies.org/introduction.html).\n",
        "\n",
        "## Tasks Overview\n",
        "\n",
        "#### Part 1: Dataset Preparation\n",
        "- **Objective**: Get the Universal Dependencies dataset ready for the tagging tasks.\n",
        "- **Activities**: Download, preprocess, and format the dataset.\n",
        "\n",
        "#### Part 2: HMM Tagger\n",
        "- **Objective**: Create and assess a POS tagger using the Hidden Markov Model.\n",
        "- **Activities**: Construct, train, and evaluate the HMM tagger.\n",
        "\n",
        "#### Part 3: Feed-Forward Neural Network Tagger\n",
        "- **Objective**: Build a POS tagger using a feed-forward neural network with word embeddings.\n",
        "- **Activities**: Develop and train the model in PyTorch, then evaluate its effectiveness.\n",
        "\n",
        "#### Part 4: NLTK MEMM Tagger\n",
        "- **Objective**: Implement and test a MEMM-based POS tagger using NLTK.\n",
        "- **Activities**: Train the MEMM tagger, then evaluate its performance.\n",
        "\n",
        "#### Part 5: Models' Comparison\n",
        "- **Objective**: Evaluate and contrast the performance of different models.\n",
        "- **Activities**: Address two open-ended questions.\n",
        "\n",
        "\n",
        "## Your Implementation\n",
        "\n",
        "Please create a local copy of this template Colab's Notebook:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FfXDvRMALIsd-IzPdf_Fn92OLVHjSY9I#scrollTo=JCFoQxO0H0yk)\n",
        "\n",
        "The assignment's instructions are there; follow the notebook.\n",
        "\n",
        "## Submission\n",
        "- **Notebook Link**: Add the URL to your assignment's notebook in the `notebook_link.txt` file, following the format provided in the example.\n",
        "- **Access**: Ensure the link has edit permissions enabled to allow modifications if needed.\n",
        "- **Deadline**: <font color='green'>21/05/2024</font>.\n",
        "- **Platform**: Continue using GitHub for submissions. Push your project to the team repository and monitor the test results under the actions section.\n",
        "\n",
        "Good Luck ðŸ¤—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8iWKz8IfKi5s"
      },
      "outputs": [],
      "source": [
        "# Prerequisite: Install the conllutils library before proceeding with the tasks\n",
        "!pip install --q conllutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iRm7zcfq56HF"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import random\n",
        "import operator\n",
        "import json\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "# Data Handling and Numerical Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "# from google.colab import files\n",
        "\n",
        "# Machine Learning and Evaluation Libraries\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# Natural Language Processing Libraries\n",
        "import nltk\n",
        "from nltk.tag import tnt\n",
        "from nltk.metrics import ConfusionMatrix, precision, recall, f_measure\n",
        "\n",
        "# Deep Learning Libraries (PyTorch)\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# CoNLL Utilities for Data Handling\n",
        "import conllutils\n",
        "import gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu"
      },
      "source": [
        "# Part 1 - Dataset Preparation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTv6rMt0oIw9"
      },
      "source": [
        "For each package you use, set the random seed to 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PtM4HY2LoYDn"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "# Set the random seed for Python\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set the random seed for numpy\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set the random seed for pytorch\n",
        "th.manual_seed(SEED)\n",
        "\n",
        "# If using CUDA (for GPU operations)\n",
        "th.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuvbl0hooXXx"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "### Step 1: Access the Dataset\n",
        "The GUM dataset, which is part of the English corpora under Universal Dependencies, is specifically curated for academic and research purposes. You can download the dataset directly from the following GitHub repository:\n",
        "\n",
        "[UD English-GUM](https://github.com/UniversalDependencies/UD_English-GUM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nsZsyTVC6Sw0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'UD_English-GUM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_English-GUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "airorSOzSRm_"
      },
      "source": [
        "### Step 2: Reading the Data\n",
        "We will use the (train/dev/test) files:\n",
        "\n",
        "\n",
        "```\n",
        "UD_English-GUM/en_gum-ud-train.conllu\n",
        "UD_English-GUM/en_gum-ud-dev.conllu\n",
        "UD_English-GUM/en_gum-ud-test.conllu\n",
        "```\n",
        "\n",
        "## CoNLL-U Format\n",
        "They are all formatted in the CoNLL-U format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
        "\n",
        "## Task: Create a Read Data Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `read_data` that:\n",
        "- Takes a file path to a `.conllu` file as input.\n",
        "- Returns a list of lists, where each inner list represents a sentence.\n",
        "- Each sentence is composed of tuples containing the word ('form') and its corresponding Universal POS tag ('upos').\n",
        "\n",
        "The word is located in the column named 'form' and the POS tag in the column named 'upos' of the CoNLL-U format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lkepUJYENXPq"
      },
      "outputs": [],
      "source": [
        "DataType = list[list[tuple[str, str]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BYt7poZ3V2LV"
      },
      "outputs": [],
      "source": [
        "from conllutils import pipe\n",
        "UNK = '<<UNK>>'\n",
        "def read_data(filepath: str, normalize_data=False) -> DataType:\n",
        "    \"\"\"\n",
        "    Reads a CoNLL-U formatted file and extracts sentences as lists of (word, POS tag) tuples.\n",
        "\n",
        "    Args:\n",
        "    filepath (str): The path to the .conllu file to be read.\n",
        "    normalize_data (bool): Whether to apply data normalization and transformation steps.\n",
        "\n",
        "    Returns:\n",
        "    List[List[Tuple[str, str]]]: A list of sentences, where each sentence is a list of tuples containing the word and its POS tag.\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    \n",
        "    p = pipe()\n",
        "    # p.only_words()\n",
        "    if normalize_data:\n",
        "        NUM_REGEX = r'[0-9]+|[0-9]+\\.[0-9]+|[0-9]+[0-9,]+'\n",
        "        NUM_FORM = '__number__'\n",
        "        p.only_universal_deprel()\n",
        "        p.upos_feats()\n",
        "        # p.lowercase('form')\n",
        "        p.replace('form', NUM_REGEX, NUM_FORM)\n",
        "\n",
        "    train_data = pipe().read_conllu(filepath).pipe(p).collect()\n",
        "\n",
        "    for s in train_data:\n",
        "        pos_list = []\n",
        "        for x in s:\n",
        "            try:\n",
        "                upos = x.upos\n",
        "            except:\n",
        "                upos = UNK\n",
        "            if x.form == \"_\":\n",
        "                x.form = UNK\n",
        "            pos_list.append((x.form, upos))\n",
        "        output.append(pos_list)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XmRpi7gXknQC"
      },
      "outputs": [],
      "source": [
        "# Run this block once `read_data` is implemented.\n",
        "train_dataset = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n",
        "dev_dataset = read_data(\"UD_English-GUM/en_gum-ud-dev.conllu\")\n",
        "test_dataset = read_data(\"UD_English-GUM/en_gum-ud-test.conllu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2E18JUi2YeD"
      },
      "source": [
        "## Task: Create a Vocabulary Generation Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `generate_vocabs` for mapping words and tags into unique numbers so that we can use them in the tagging algorithms you will implement below. The function:\n",
        "- Takes a list of sentences as input, where each sentence is composed of tuples containing a word and its corresponding Universal POS tag.\n",
        "- Returns a tuple of two dictionaries:\n",
        "  - The first dictionary maps each unique word to a unique integer.\n",
        "  - The second dictionary maps each unique POS tag to a unique integer.\n",
        "\n",
        "Each word and POS tag should be mapped starting from 0, with each new word or tag encountered receiving the next sequential integer. This function is essential for converting textual data into a numerical format that can be used by tagging algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j47ImFVyyoO4"
      },
      "outputs": [],
      "source": [
        "def generate_vocabs(datasets: list[DataType]) -> tuple[dict[str, int], dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Generates vocabularies mapping words and tags to unique indices from a list of datasets.\n",
        "\n",
        "    Args:\n",
        "    datasets (list): A list of datasets where each dataset contains sentences formatted as lists of (word, tag) tuples.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[dict[str, int], dict[str, int]]: A tuple of two dictionaries:\n",
        "        - The first dictionary maps each unique word to a unique integer.\n",
        "        - The second dictionary maps each unique POS tag to a unique integer.\n",
        "        Each dictionary includes a special '<<UNK>>' entry mapped to 0 to handle unknown words or tags.\n",
        "    \"\"\"\n",
        "    words_vocab = {\"<<UNK>>\": 0}\n",
        "    tags_vocab = {\"<<UNK>>\": 0}\n",
        "    word_index = 1\n",
        "    tag_index = 1\n",
        "    for dataset in datasets:\n",
        "        for sentence in dataset:\n",
        "            for word, tag in sentence:\n",
        "                if tag == UNK:\n",
        "                    continue\n",
        "                if word not in words_vocab:\n",
        "                    words_vocab[word] = word_index\n",
        "                    word_index += 1\n",
        "                if tag not in tags_vocab:\n",
        "                    tags_vocab[tag] = tag_index\n",
        "                    tag_index += 1\n",
        "\n",
        "    return words_vocab, tags_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FwzutNN21Z6m"
      },
      "outputs": [],
      "source": [
        "# Run this block once `generate_vocabs` is implemented.\n",
        "words_vocab, tags_vocab = generate_vocabs([train_dataset, dev_dataset, test_dataset])\n",
        "reversed_words_vocab = {v: k for k, v in words_vocab.items()}\n",
        "reversed_tags_vocab = {v: k for k, v in tags_vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X"
      },
      "source": [
        "# Part 2 - HMM Tagger\n",
        "\n",
        "### Task Description\n",
        "Implement a class `HMMTagger` to perform POS tagging using a Hidden Markov Model (HMM).\n",
        "\n",
        "### Class Methods\n",
        "- `fit`: This method should compute the transition probabilities matrix (A), emission probabilities matrix (B), and initial state probabilities vector (Pi) based on the training data. These matrices should reflect probabilities of transitions between tags, emissions of words given tags, and initial tag probabilities, respectively.\n",
        "- `inference`: Implement this method to predict the best tag sequence for a given input sentence using the Viterbi decoding algorithm. The Viterbi algorithm is provided below.\n",
        "### Additional Guidance\n",
        "1. **Use of Vocabularies**: Utilize the vocabularies generated in Part 1. These should include a special entry for unknown words and tags (`<<UNK>>` at index 0). The indices in your vocabularies will correspond to the rows and columns of your A, B, and Pi matrices (np.array).\n",
        "2. **Smoothing**: Apply Add-One Smoothing to all probability calculations to avoid zero probabilities. This technique adjusts the frequency counts for each observed event by adding one to each count.\n",
        "3. **Word Conversion**: During inference, convert each word of the input sentence into its corresponding index using the word vocabulary. If a word is not found, use the index for `<<UNK>>`.\n",
        "\n",
        "### Implementation Tips\n",
        "- You can use the vocab dictionaries directly, no need to pass them as a parameter to the functions.\n",
        "- You may add functions to `HMMTagger` as needed.\n",
        "- Ensure that your A, B, and Pi matrices handle unseen words/tags gracefully using the `<<UNK>>` index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TpH7GuiQ9L6W"
      },
      "outputs": [],
      "source": [
        "class HMMTagger:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the HMMTagger class with necessary attributes.\n",
        "        \"\"\"\n",
        "        self._Ï€ = np.zeros(len(tags_vocab))  # Initial state probabilities\n",
        "        self._A = np.zeros((len(tags_vocab), len(tags_vocab)))  # Transition probabilities\n",
        "        self._B = np.zeros((len(tags_vocab), len(words_vocab)))  # Emission probabilities\n",
        "\n",
        "    def fit(self, dataset: DataType):\n",
        "        \"\"\"\n",
        "        Trains the HMM model on the provided dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): The training dataset containing sentences as lists of (word, tag) tuples.\n",
        "        \"\"\"\n",
        "        # Compute initial state probabilities\n",
        "        for sentence in dataset:\n",
        "            self._Ï€[tags_vocab[sentence[0][1]]] += 1\n",
        "\n",
        "        self._Ï€ = (self._Ï€ + 1) / (len(dataset) + len(tags_vocab))\n",
        "\n",
        "        # Compute transition probabilities\n",
        "        for sentence in dataset:\n",
        "            for i in range(len(sentence) - 1):\n",
        "                tag1 = tags_vocab[sentence[i][1]]\n",
        "                tag2 = tags_vocab[sentence[i + 1][1]]\n",
        "                self._A[tag1, tag2] += 1\n",
        "\n",
        "        self._A = (self._A + 1) / (np.sum(self._A, axis=1, keepdims=True) + len(tags_vocab))\n",
        "\n",
        "        # Compute emission probabilities\n",
        "        for sentence in dataset:\n",
        "            for word, tag in sentence:\n",
        "                word_index = words_vocab.get(word, 0)\n",
        "                tag_index = tags_vocab[tag]\n",
        "                self._B[tag_index, word_index] += 1\n",
        "\n",
        "        # Apply add-one smoothing and normalize\n",
        "        self._B = (self._B + 1) / (np.sum(self._B, axis=1, keepdims=True) + len(words_vocab))\n",
        "\n",
        "    def inference(self, sentence: list) -> list[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Predicts the best tag sequence for a given input sentence using the Viterbi decoding algorithm.\n",
        "\n",
        "        Args:\n",
        "            sentence (list): The sentence to tag, as a list of words.\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[str, str]]: Each word in the input sentence paired with its predicted tag.\n",
        "        \"\"\"\n",
        "        # Initialize the Viterbi algorithm\n",
        "        T = len(sentence)\n",
        "        N = len(tags_vocab)\n",
        "        viterbi = np.zeros((T, N))\n",
        "        backpointer = np.zeros((T, N), dtype=int)\n",
        "\n",
        "        # Initialize the first row in the Viterbi matrix\n",
        "        for s in range(N):\n",
        "            viterbi[0, s] = self._Ï€[s] * self._B[s, words_vocab.get(sentence[0], 0)]\n",
        "\n",
        "        # Iterate over the rest of the words in the sentence\n",
        "        for t in range(1, T):\n",
        "            for s in range(N):\n",
        "                viterbi[t, s] = np.max(viterbi[t - 1] * self._A[:, s]) * self._B[s, words_vocab.get(sentence[t], 0)]\n",
        "                backpointer[t, s] = np.argmax(viterbi[t - 1] * self._A[:, s])\n",
        "\n",
        "        # Backtrack to find the best tag sequence\n",
        "        best_path = [np.argmax(viterbi[T - 1])]\n",
        "        for t in range(T - 1, 0, -1):\n",
        "            best_path.append(backpointer[t, best_path[-1]])\n",
        "        best_path.reverse()\n",
        "\n",
        "        return [(sentence[i], reversed_tags_vocab[best_path[i]]) for i in range(T)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni-3uwVmXiKB"
      },
      "source": [
        "### Optional Task: Implement the Viterbi Algorithm\n",
        "Implement the `viterbi` function to perform POS tagging using the Viterbi decoding algorithm. This algorithm finds the most probable sequence of hidden states (POS tags in this case) given a sequence of observations (words in the sentence).\n",
        "\n",
        "### Instructions\n",
        "- **Pre-Implemented Code**: We provide a pre-implemented version of the Viterbi algorithm for your convenience. This implementation is fully functional and can be used directly in your HMM tagger.\n",
        "  \n",
        "- **Implementation Challenge**: Although a pre-implemented version is available, we encourage you to implement the Viterbi algorithm yourself. Doing so will help you understand the dynamics of dynamic programming in the context of POS tagging. Follow the pseudocode provided in the lecture slides to develop your own version of the algorithm.\n",
        "\n",
        "### Steps for Implementation\n",
        "1. **Understand the Pseudocode**: Review the pseudocode provided in the slides from the class. Ensure you understand each step of the algorithm, including how the probabilities are updated and the backtracking process to recover the state sequence.\n",
        "  \n",
        "2. **Implement the Function**: Using the pseudocode as a guide, write your own `viterbi` function. Consider the matrices (A, B, and Pi) you prepared in the HMMTagger class as inputs along with the sequence of observations (word indices for a sentence).\n",
        "  \n",
        "3. **Test Your Implementation**: After implementing the function, test it with known inputs to ensure it produces the correct sequence of tags. Compare the results with those obtained from the pre-implemented version to validate your implementation.\n",
        "\n",
        "### Additional Tips\n",
        "- **Handle Edge Cases**: Consider edge cases such as very short sentences, sentences containing many unknown words, and varying sentence structures.\n",
        "- **Optimization**: Once your basic implementation is correct, think about potential optimizations to improve the efficiency of your code, especially if you are processing large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DR6KJW2F9yqt"
      },
      "outputs": [],
      "source": [
        "def viterbi(word_list: list, A: np.ndarray, B: np.ndarray, Pi: np.ndarray):\n",
        "    \"\"\"\n",
        "    Executes the Viterbi algorithm to find the most likely state sequence given a sequence of observations.\n",
        "\n",
        "    The Viterbi algorithm is a dynamic programming algorithm used to compute the most likely sequence of hidden states\n",
        "    (called the Viterbi path) that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).\n",
        "\n",
        "    Args:\n",
        "        word_list (list): A list of indices corresponding to observed words.\n",
        "        A (numpy.ndarray): The state transition probability matrix of shape (num_states, num_states) where A[i][j] is the probability of transitioning from state i to state j.\n",
        "        B (numpy.ndarray): The emission probability matrix of shape (num_states, num_vocabulary) where B[i][j] is the probability of emitting symbol j from state i.\n",
        "        Pi (numpy.ndarray): The initial state probability vector of length num_states where Pi[i] is the probability of starting in state i.\n",
        "\n",
        "    Returns:\n",
        "        list: The most likely sequence of states (as indices) for the given sequence of observations.\n",
        "\n",
        "    The function uses three main steps: initialization, recursion, and termination:\n",
        "    - **Initialization**: Set the initial probabilities of being in each state.\n",
        "    - **Recursion**: For each subsequent observation, compute probabilities of each state based on the observation and transition probabilities from previous states.\n",
        "    - **Termination**: Backtrace to determine the most probable path through the state space.\n",
        "    \"\"\"\n",
        "    # Number of states\n",
        "    num_states = len(A)\n",
        "    # Length of the observed sequence\n",
        "    T = len(word_list)\n",
        "\n",
        "    # Create the path probability matrix V\n",
        "    V = [[0 for _ in range(num_states)] for _ in range(T)]\n",
        "    # Create a path backpointer matrix to store the argmax indices\n",
        "    path = [[0 for _ in range(num_states)] for _ in range(T)]\n",
        "\n",
        "    # Initialization step\n",
        "    for s in range(num_states):\n",
        "        V[0][s] = Pi[s] * B[s][word_list[0]]\n",
        "        path[0][s] = 0\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, T):\n",
        "        for s in range(num_states):\n",
        "            # Find the state with the maximum probability to reach state s\n",
        "            max_tr_prob = V[t-1][0] * A[0][s]\n",
        "            prev_state_selected = 0\n",
        "            for prev_state in range(1, num_states):\n",
        "                tr_prob = V[t-1][prev_state] * A[prev_state][s]\n",
        "                if tr_prob > max_tr_prob:\n",
        "                    max_tr_prob = tr_prob\n",
        "                    prev_state_selected = prev_state\n",
        "            # Multiply the max probability by the probability of observing the symbol at state s\n",
        "            max_prob = max_tr_prob * B[s][word_list[t]]\n",
        "            V[t][s] = max_prob\n",
        "            path[t][s] = prev_state_selected\n",
        "\n",
        "    # Termination step\n",
        "    # Find the best path by looking for the maximum probability in the last column\n",
        "    opt = []\n",
        "    max_prob = -1\n",
        "    best_last_state = 0\n",
        "    for s in range(num_states):\n",
        "        if V[T-1][s] > max_prob:\n",
        "            max_prob = V[T-1][s]\n",
        "            best_last_state = s\n",
        "    opt.append(best_last_state)\n",
        "\n",
        "    # Follow the back pointers to decode the best path\n",
        "    previous = best_last_state\n",
        "    for t in range(T-1, 0, -1):\n",
        "        opt.insert(0, path[t][previous])\n",
        "        previous = path[t][previous]\n",
        "\n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Bkj25vm2knij"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example for using Viterbi algorithm (from the course slides)\n",
        "A = np.array([[0.3, 0.7], [0.2, 0.8]])\n",
        "B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])\n",
        "Pi = np.array([0.4, 0.6])\n",
        "\n",
        "viterbi([0, 3, 2, 0], A, B, Pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKUWMhyUgXy"
      },
      "source": [
        "## Train & Evaluate\n",
        "\n",
        "## Task: Implement the Train and Evaluate Tagger Function\n",
        "\n",
        "### Function Specification\n",
        "Create a function named `train_evaluate_tagger` that combines the training and evaluation processes for a tagging algorithm. This integrated function should:\n",
        "- **Input**:\n",
        "  - **tagger** (`HMMTagger` or `EmbeddingsTagger`): The POS tagger to be trained and evaluated.\n",
        "  - **train_dataset** (`List[List[Tuple[str, str]]]`): The dataset on which the tagger will be trained. Training is executed only if the `train` flag is set to True.\n",
        "  - **eval_dataset** (`List[List[Tuple[str, str]]]`): The dataset used for evaluating the tagger's performance.\n",
        "  - **train** (`bool`): A boolean flag indicating whether the training phase should be executed. If set to True, the `fit` method of the tagger will be called before evaluation.\n",
        "- **Functionality**:\n",
        "  - Train the tagger using the `train_dataset`.\n",
        "  - Evaluate the trained tagger on the `test_dataset` to compute performance metrics.\n",
        "  - Return the following evaluation metrics: accuracy, precision, recall, and F1-score.\n",
        "- **Outputs**: A tuple consisting of three elements: precision, recall, F1-score, as returned by the `precision_recall_fscore_support` function from the `sklearn.metrics` module, when called with the parameter `average=\"macro\"`.\n",
        "* Note the the support metric is missed (Why?).\n",
        "\n",
        "\n",
        "### Evaluation Metrics\n",
        "Upon execution, the `train_evaluate_tagger` function provides the following metrics:\n",
        "- **Macro-average Precision, Recall, and F1 Score**: These scores are calculated over all tags to assess the overall effectiveness of the tagger in recognizing the correct tags across different types of words.\n",
        "- **Performance Breakdown by Tag**: Detailed metrics for each tag, helping to identify which tags are most and least accurately predicted.\n",
        "- **Overall Word-Level Accuracy**: Measures the percentage of words correctly tagged by the tagger across the entire evaluation dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Vumzm2lB7p1-"
      },
      "outputs": [],
      "source": [
        "def train_evaluate_tagger(tagger, train_dataset: DataType=train_dataset, eval_dataset: DataType=test_dataset, train=True):\n",
        "    \"\"\"\n",
        "    Trains (optional) and evaluates a POS tagger, returning performance metrics.\n",
        "\n",
        "    This function trains the given tagger if specified, and evaluates it on a provided dataset.\n",
        "    It calculates and returns the macro-average precision, recall, and F1-score.\n",
        "\n",
        "    Args:\n",
        "        tagger (HMMTagger or EmbeddingsTagger): The POS tagger to be trained and evaluated.\n",
        "        train_dataset (List[List[Tuple[str, str]]]): The dataset to train the tagger.\n",
        "        eval_dataset (List[List[Tuple[str, str]]]): The dataset to evaluate the tagger.\n",
        "        train (bool): A flag indicating whether the tagger should be trained.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing average precision, recall, and F1-score.\n",
        "\n",
        "    The function also prints a classification report for detailed performance analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    true_tags = []\n",
        "    predicted_tags = []\n",
        "\n",
        "    if train:\n",
        "        tagger.fit(train_dataset)\n",
        "\n",
        "    for sentence in eval_dataset:\n",
        "        words = [word for word, _ in sentence]\n",
        "        true_tags.extend([tag for _, tag in sentence])\n",
        "        predicted_tags.extend([tag for _, tag in tagger.inference(words)])\n",
        "\n",
        "    # Compute precision, recall, F1-score, and support for each class\n",
        "    precision, recall, f1_score, support = precision_recall_fscore_support(true_tags, predicted_tags, average=None)\n",
        "\n",
        "    # Display classification report\n",
        "    report = classification_report(true_tags, predicted_tags)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Return the macro-average metrics\n",
        "    avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(true_tags, predicted_tags, average='macro')\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "l0uM2tozQmpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     <<UNK>>       0.77      1.00      0.87       325\n",
            "         ADJ       0.79      0.74      0.77      1622\n",
            "         ADP       0.86      0.97      0.91      2481\n",
            "         ADV       0.84      0.74      0.79      1114\n",
            "         AUX       0.80      0.95      0.87      1189\n",
            "       CCONJ       0.98      0.97      0.98       839\n",
            "         DET       0.82      0.97      0.89      2111\n",
            "        INTJ       0.68      0.64      0.66       163\n",
            "        NOUN       0.83      0.82      0.82      4239\n",
            "         NUM       0.72      0.65      0.68       440\n",
            "        PART       0.85      0.77      0.81       519\n",
            "        PRON       0.81      0.97      0.89      1746\n",
            "       PROPN       0.83      0.46      0.59      1628\n",
            "       PUNCT       0.92      1.00      0.96      3027\n",
            "       SCONJ       0.84      0.54      0.66       340\n",
            "         SYM       0.50      0.29      0.36        35\n",
            "        VERB       0.87      0.75      0.81      2480\n",
            "           X       0.50      0.09      0.16        32\n",
            "\n",
            "    accuracy                           0.85     24330\n",
            "   macro avg       0.79      0.74      0.75     24330\n",
            "weighted avg       0.84      0.85      0.84     24330\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train (optional) and Evaluate your HMMTagger here\n",
        "hmmTagger = HMMTagger()\n",
        "precision_hmm, recall_hmm, f1_hmm = train_evaluate_tagger(hmmTagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3fPdEWBetDE"
      },
      "source": [
        "## Part 3 - POS Tagging with Pre-trained Word Embeddings\n",
        "\n",
        "### Task Overview\n",
        "Develop a feed-forward neural network for Part-of-Speech (POS) tagging using the PyTorch framework. This tagger leverages pre-trained word embeddings from the word2vec Google News dataset, enhancing the semantic understanding of words compared to traditional tagging methods.\n",
        "\n",
        "### Task 1: Initialization of Embeddings (`init_embeddings`)\n",
        "Create an `init_embeddings` method within the `EmbeddingsTagger` class to load and set up pre-trained word embeddings:\n",
        "- Load the Google News word vectors from a specified file path.\n",
        "- Initialize a matrix to hold these embeddings where each word in your vocabulary is represented by a vector. For words not in the pre-trained model, initialize their vectors randomly.\n",
        "-  <font color='red'>**NOTE:** Before you start implementing the init_embeddings method, make sure to create your own copy of the pre-trained embeddings (.bin file) in your Google Drive from [the following Google Drive folder](https://drive.google.com/drive/folders/1-HcSBfqaX0PCFiT8TsiYjFZRQJRm2R5v?usp=sharing). You will need to use the gensim library to load this file.</font>\n",
        "\n",
        "### Task 2: Implementation of `EmbeddingsTagger` Class\n",
        "Extend PyTorch's `nn.Module` to implement the `EmbeddingsTagger` class. This class should utilize the embeddings matrix and include:\n",
        "- An embedding layer that is initialized with the pre-trained embeddings.\n",
        "- A linear layer to combine embeddings of the current and previous words with a one-hot encoded previous tag to predict the current tag.\n",
        "- A `forward` method that outlines the data flow through the network.\n",
        "\n",
        "### Task 3: Model Training and Evaluation (`fit` and `inference` Methods)\n",
        "Implement training and inference functionalities within the `EmbeddingsTagger` class:\n",
        "- **Train Method (`fit`)**: Set up the model training using the specified training dataset. This includes iterating through the dataset, applying the model to predict tags, and updating model parameters based on the loss computation.\n",
        "- **Inference Method (`inference`)**: Configure the model to predict tags on a new dataset, assessing the model's effectiveness on unseen data.\n",
        "\n",
        "### Model Performance Monitoring\n",
        "Utilize the `train_evaluate_tagger` function to oversee the training process and evaluate the model:\n",
        "- Configure this function to handle model training with appropriate optimizer and loss settings.\n",
        "- Monitor and report the model's performance metrics during training and on a test dataset to ensure effective tagging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOGqtQtyUsA9"
      },
      "source": [
        "### Google News pre-trained embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XROdy8sHZ-Qu",
        "outputId": "71cb2475-c88c-4b95-f167-da30c5854a7d"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_to_google_news_vectors = 'GoogleNews-vectors-negative300.bin'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_google_news_vectors, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Ljrof4K3Va70"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class EmbeddingsTagger(nn.Module):\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Initializes an embeddings-based POS tagger that uses word embeddings from a pre-trained model.\n",
        "\n",
        "        Args:\n",
        "            device (str): The device (cpu or cuda) the model should operate on for tensor operations.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        super(EmbeddingsTagger, self).__init__()\n",
        "        self.embedding_dim = 300  # Dimension of Google News embeddings\n",
        "        self.tagset_size = len(tags_vocab)\n",
        "        self.word_embeddings = self.init_embeddings()\n",
        "        self.to(device)\n",
        "\n",
        "\n",
        "        # The input dimension to the linear layer is twice the embedding_dim (for two words)\n",
        "        # plus tagset_size (for the one-hot encoded previous tag)\n",
        "        self.linear_layer = nn.Linear(2 * self.embedding_dim + self.tagset_size, self.tagset_size).to(self.device)\n",
        "\n",
        "\n",
        "    def init_embeddings(self) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Loads word embeddings from a pre-trained Google News model and initializes an embedding layer.\n",
        "\n",
        "        Returns:\n",
        "            nn.Embedding: A PyTorch embedding layer with the pre-trained word embeddings.\n",
        "        \"\"\"\n",
        "        # Specify the path to the Google News model in your Google Drive\n",
        "        # path_to_google_news_vectors = '/content/drive/My Drive/GoogleNews-vectors-negative300.bin'\n",
        "        path_to_google_news_vectors = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        # Load Google News Vectors\n",
        "        print(\"Loading Google News Vectors...\")\n",
        "        # model = gensim.models.KeyedVectors.load_word2vec_format(path_to_google_news_vectors, binary=True)\n",
        "\n",
        "        # Prepare a matrix to hold the embeddings\n",
        "        embedding_matrix = np.zeros((len(words_vocab), self.embedding_dim))  # Ensure 'words_vocab' maps words to indices, 300 is the dimension of embeddings\n",
        "\n",
        "        # Fill the matrix with embeddings from the pre-trained model\n",
        "        for word, index in tqdm(words_vocab.items(), desc=\"Loading Embeddings\"):\n",
        "            if word in model:\n",
        "                embedding_matrix[index] = model[word]\n",
        "            else:\n",
        "                # For words not in the pre-trained model, initialize their vectors randomly\n",
        "                embedding_matrix[index] = np.random.uniform(-0.15, 0.15, self.embedding_dim)\n",
        "\n",
        "        # Initialize the embedding layer with the pre-trained embeddings\n",
        "        return nn.Embedding.from_pretrained(th.FloatTensor(embedding_matrix), freeze=True)\n",
        "\n",
        "    def forward(self, current_word_index: int, previous_word_index: int, prev_tag_one_hot: th.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of the tagger to compute logits for each tag.\n",
        "\n",
        "        Args:\n",
        "            current_word_index (int): Index of the current word in the vocabulary.\n",
        "            previous_word_index (int): Index of the previous word in the vocabulary.\n",
        "            prev_tag_one_hot (th.tensor): One-hot encoded tensor of the previous tag.\n",
        "\n",
        "        Returns:\n",
        "            th.tensor: Logits for each tag.\n",
        "        \"\"\"\n",
        "        current_word_embedding = self.word_embeddings(th.LongTensor([current_word_index]).to(self.device))\n",
        "        previous_word_embedding = self.word_embeddings(th.LongTensor([previous_word_index]).to(self.device))\n",
        "        tag_scores = self.linear_layer(th.cat((current_word_embedding, previous_word_embedding, prev_tag_one_hot[None, :]), 1))\n",
        "        tag_scores = nn.functional.log_softmax(tag_scores, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "    def fit(self, dataset=list(train_dataset), epochs=1):\n",
        "        \"\"\"\n",
        "        Trains the tagger on the provided dataset for a specified number of epochs.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): The dataset to train the model on.\n",
        "            epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        loss_function = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
        "\n",
        "        # preparing instances for training\n",
        "        instances = []\n",
        "\n",
        "        for sentence in dataset:\n",
        "            for i in range(len(sentence)):\n",
        "                current_word, current_tag = sentence[i]\n",
        "                previous_word, previous_tag = sentence[i - 1] if i > 0 else (UNK, UNK)\n",
        "                prev_tag_one_hot = th.zeros(self.tagset_size)\n",
        "                prev_tag_one_hot[tags_vocab[previous_tag]] = 1\n",
        "                prev_tag_one_hot = prev_tag_one_hot.to(self.device)\n",
        "                target_tag = th.LongTensor([tags_vocab[current_tag]]).to(self.device)\n",
        "                instances.append((words_vocab.get(current_word,0), words_vocab.get(previous_word, 0), prev_tag_one_hot, target_tag))\n",
        "\n",
        "        loss_c = 0\n",
        "        for epoch in range(epochs):\n",
        "            for index, (current_word, previous_word, prev_tag_one_hot, target_tag) in enumerate(instances):\n",
        "                self.zero_grad()\n",
        "                tag_scores = self(current_word, previous_word, prev_tag_one_hot)\n",
        "                loss = loss_function(tag_scores, target_tag)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_c += loss.item()\n",
        "                if index % 1000 == 0:\n",
        "                    print(f\"Epoch: {epoch}, Instance: {index}, Loss: {loss_c / 1000}\")\n",
        "                    loss_c = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def inference(self, sentence: list) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Predicts the tags for each word in a given sentence.\n",
        "\n",
        "        Args:\n",
        "            sentence (list): The sentence to tag, given as a list of words.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of tuples containing each word and its predicted tag.\n",
        "        \"\"\"\n",
        "\n",
        "        predicted_tags = []\n",
        "        prev_tag_one_hot = th.zeros(self.tagset_size)\n",
        "\n",
        "        for i in range(len(sentence)):\n",
        "            current_word = sentence[i]\n",
        "            previous_word = sentence[i - 1] if i > 0 else UNK\n",
        "            prev_tag_one_hot = th.zeros(self.tagset_size)\n",
        "            if i > 0:\n",
        "                prev_tag_one_hot[tags_vocab[predicted_tags[-1]]] = 1\n",
        "            else:\n",
        "                prev_tag_one_hot[tags_vocab[UNK]] = 1\n",
        "            prev_tag_one_hot = prev_tag_one_hot.to(self.device)\n",
        "            current_word_index = words_vocab.get(current_word, 0) \n",
        "            previous_word_index = words_vocab.get(previous_word, 0)\n",
        "            tag_scores = self(current_word_index, previous_word_index, prev_tag_one_hot)\n",
        "            _, predicted_tag_index = th.max(tag_scores, 1)\n",
        "            predicted_tags.append(reversed_tags_vocab[predicted_tag_index.item()])\n",
        "\n",
        "        return list(zip(sentence, predicted_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l_IoQJ2PXOaZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Google News Vectors...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19873/19873 [00:00<00:00, 290164.08it/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model with pre-trained embeddings\n",
        "# device = th.device(\"cuda\")\n",
        "embeddings_tagger = EmbeddingsTagger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8IDlGSTbd-CZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Instance: 0, Loss: 0.0029696733951568603\n",
            "Epoch: 0, Instance: 1000, Loss: 0.8817677881898126\n",
            "Epoch: 0, Instance: 2000, Loss: 0.4722708818251358\n",
            "Epoch: 0, Instance: 3000, Loss: 0.47639717898057277\n",
            "Epoch: 0, Instance: 4000, Loss: 0.4168627592502658\n",
            "Epoch: 0, Instance: 5000, Loss: 0.4054903111952659\n",
            "Epoch: 0, Instance: 6000, Loss: 0.4050997260645649\n",
            "Epoch: 0, Instance: 7000, Loss: 0.44265225039920303\n",
            "Epoch: 0, Instance: 8000, Loss: 0.43901026784629404\n",
            "Epoch: 0, Instance: 9000, Loss: 0.504856695339047\n",
            "Epoch: 0, Instance: 10000, Loss: 0.3849156033152536\n",
            "Epoch: 0, Instance: 11000, Loss: 0.3516271889523478\n",
            "Epoch: 0, Instance: 12000, Loss: 0.3878160398738116\n",
            "Epoch: 0, Instance: 13000, Loss: 0.4053505274703925\n",
            "Epoch: 0, Instance: 14000, Loss: 0.430211528964849\n",
            "Epoch: 0, Instance: 15000, Loss: 0.44945159015906366\n",
            "Epoch: 0, Instance: 16000, Loss: 0.3802920091832018\n",
            "Epoch: 0, Instance: 17000, Loss: 0.3574570304806435\n",
            "Epoch: 0, Instance: 18000, Loss: 0.39075307706811935\n",
            "Epoch: 0, Instance: 19000, Loss: 0.5562758762523584\n",
            "Epoch: 0, Instance: 20000, Loss: 0.44689407657325025\n",
            "Epoch: 0, Instance: 21000, Loss: 0.45021267725081693\n",
            "Epoch: 0, Instance: 22000, Loss: 0.5689552118872483\n",
            "Epoch: 0, Instance: 23000, Loss: 0.44011857561018164\n",
            "Epoch: 0, Instance: 24000, Loss: 0.6344624332353332\n",
            "Epoch: 0, Instance: 25000, Loss: 0.4656451786725379\n",
            "Epoch: 0, Instance: 26000, Loss: 0.42251032199706967\n",
            "Epoch: 0, Instance: 27000, Loss: 0.5544475675873728\n",
            "Epoch: 0, Instance: 28000, Loss: 0.4474846530847923\n",
            "Epoch: 0, Instance: 29000, Loss: 0.7465994629087627\n",
            "Epoch: 0, Instance: 30000, Loss: 0.5333156761451996\n",
            "Epoch: 0, Instance: 31000, Loss: 0.37161095413639195\n",
            "Epoch: 0, Instance: 32000, Loss: 0.4211968023167553\n",
            "Epoch: 0, Instance: 33000, Loss: 0.5415578256828667\n",
            "Epoch: 0, Instance: 34000, Loss: 0.44119493216151223\n",
            "Epoch: 0, Instance: 35000, Loss: 0.39414000193116777\n",
            "Epoch: 0, Instance: 36000, Loss: 0.5137990872599321\n",
            "Epoch: 0, Instance: 37000, Loss: 0.34317447388406724\n",
            "Epoch: 0, Instance: 38000, Loss: 0.3856512001639027\n",
            "Epoch: 0, Instance: 39000, Loss: 0.31863190297258426\n",
            "Epoch: 0, Instance: 40000, Loss: 0.35899693218960194\n",
            "Epoch: 0, Instance: 41000, Loss: 0.40294233412001734\n",
            "Epoch: 0, Instance: 42000, Loss: 0.3950967344731114\n",
            "Epoch: 0, Instance: 43000, Loss: 0.5534059405182643\n",
            "Epoch: 0, Instance: 44000, Loss: 0.45884962867182155\n",
            "Epoch: 0, Instance: 45000, Loss: 0.39939625590811567\n",
            "Epoch: 0, Instance: 46000, Loss: 0.43712431475119634\n",
            "Epoch: 0, Instance: 47000, Loss: 0.49957557516526063\n",
            "Epoch: 0, Instance: 48000, Loss: 0.6121255425551124\n",
            "Epoch: 0, Instance: 49000, Loss: 0.47458371885496253\n",
            "Epoch: 0, Instance: 50000, Loss: 0.47252050636547455\n",
            "Epoch: 0, Instance: 51000, Loss: 0.5536750434925536\n",
            "Epoch: 0, Instance: 52000, Loss: 0.35061934492968777\n",
            "Epoch: 0, Instance: 53000, Loss: 0.4269096690014502\n",
            "Epoch: 0, Instance: 54000, Loss: 0.441382060722508\n",
            "Epoch: 0, Instance: 55000, Loss: 0.5458070763032747\n",
            "Epoch: 0, Instance: 56000, Loss: 0.5852748801778144\n",
            "Epoch: 0, Instance: 57000, Loss: 0.4812817170039016\n",
            "Epoch: 0, Instance: 58000, Loss: 0.5058869943921529\n",
            "Epoch: 0, Instance: 59000, Loss: 0.22463105653886875\n",
            "Epoch: 0, Instance: 60000, Loss: 0.3369843392000289\n",
            "Epoch: 0, Instance: 61000, Loss: 0.33367544715844094\n",
            "Epoch: 0, Instance: 62000, Loss: 0.5708567433969328\n",
            "Epoch: 0, Instance: 63000, Loss: 0.6315436235999742\n",
            "Epoch: 0, Instance: 64000, Loss: 0.7177724503934837\n",
            "Epoch: 0, Instance: 65000, Loss: 0.6557797170958627\n",
            "Epoch: 0, Instance: 66000, Loss: 0.6181701819803304\n",
            "Epoch: 0, Instance: 67000, Loss: 0.44578446792778376\n",
            "Epoch: 0, Instance: 68000, Loss: 0.35505103363552715\n",
            "Epoch: 0, Instance: 69000, Loss: 0.5748633736996086\n",
            "Epoch: 0, Instance: 70000, Loss: 0.43781054273233744\n",
            "Epoch: 0, Instance: 71000, Loss: 0.6270226991923402\n",
            "Epoch: 0, Instance: 72000, Loss: 0.5543280185293253\n",
            "Epoch: 0, Instance: 73000, Loss: 0.4888025385070157\n",
            "Epoch: 0, Instance: 74000, Loss: 0.5180254562501243\n",
            "Epoch: 0, Instance: 75000, Loss: 0.48490506315116416\n",
            "Epoch: 0, Instance: 76000, Loss: 0.45643626759039463\n",
            "Epoch: 0, Instance: 77000, Loss: 0.43583420875086626\n",
            "Epoch: 0, Instance: 78000, Loss: 0.2828487045218575\n",
            "Epoch: 0, Instance: 79000, Loss: 0.4869570259673899\n",
            "Epoch: 0, Instance: 80000, Loss: 0.5472906963305515\n",
            "Epoch: 0, Instance: 81000, Loss: 0.4313023057880014\n",
            "Epoch: 0, Instance: 82000, Loss: 1.091477866278184\n",
            "Epoch: 0, Instance: 83000, Loss: 0.7384897032287975\n",
            "Epoch: 0, Instance: 84000, Loss: 0.4009246095519703\n",
            "Epoch: 0, Instance: 85000, Loss: 0.4516389169067796\n",
            "Epoch: 0, Instance: 86000, Loss: 0.5144451444766661\n",
            "Epoch: 0, Instance: 87000, Loss: 0.556427953022763\n",
            "Epoch: 0, Instance: 88000, Loss: 0.5807818686405264\n",
            "Epoch: 0, Instance: 89000, Loss: 0.44298711143647396\n",
            "Epoch: 0, Instance: 90000, Loss: 0.5151187111406864\n",
            "Epoch: 0, Instance: 91000, Loss: 0.643973124925956\n",
            "Epoch: 0, Instance: 92000, Loss: 0.38865839939855884\n",
            "Epoch: 0, Instance: 93000, Loss: 0.4683171213832876\n",
            "Epoch: 0, Instance: 94000, Loss: 0.5109655969992699\n",
            "Epoch: 0, Instance: 95000, Loss: 0.4146966567917612\n",
            "Epoch: 0, Instance: 96000, Loss: 0.5197068946432485\n",
            "Epoch: 0, Instance: 97000, Loss: 0.483170321531232\n",
            "Epoch: 0, Instance: 98000, Loss: 0.5322687266836892\n",
            "Epoch: 0, Instance: 99000, Loss: 0.4528886262549831\n",
            "Epoch: 0, Instance: 100000, Loss: 0.5199360078569281\n",
            "Epoch: 0, Instance: 101000, Loss: 0.6294913311716512\n",
            "Epoch: 0, Instance: 102000, Loss: 0.44730283010104466\n",
            "Epoch: 0, Instance: 103000, Loss: 0.5129738110674054\n",
            "Epoch: 0, Instance: 104000, Loss: 0.4131863881608061\n",
            "Epoch: 0, Instance: 105000, Loss: 0.41886180370286447\n",
            "Epoch: 0, Instance: 106000, Loss: 0.4723498279979887\n",
            "Epoch: 0, Instance: 107000, Loss: 0.37392353215956614\n",
            "Epoch: 0, Instance: 108000, Loss: 0.43212261010394376\n",
            "Epoch: 0, Instance: 109000, Loss: 0.39504334556821563\n",
            "Epoch: 0, Instance: 110000, Loss: 0.7102415019793308\n",
            "Epoch: 0, Instance: 111000, Loss: 0.5325110611708433\n",
            "Epoch: 0, Instance: 112000, Loss: 0.2493523906524127\n",
            "Epoch: 0, Instance: 113000, Loss: 0.3778657623253564\n",
            "Epoch: 0, Instance: 114000, Loss: 0.47435560522036085\n",
            "Epoch: 0, Instance: 115000, Loss: 0.4091495113386307\n",
            "Epoch: 0, Instance: 116000, Loss: 0.7129685049750911\n",
            "Epoch: 0, Instance: 117000, Loss: 0.4887414529569514\n",
            "Epoch: 0, Instance: 118000, Loss: 0.328410457305357\n",
            "Epoch: 0, Instance: 119000, Loss: 0.5443215990137885\n",
            "Epoch: 0, Instance: 120000, Loss: 0.4398739175354197\n",
            "Epoch: 0, Instance: 121000, Loss: 0.4236517610474998\n",
            "Epoch: 0, Instance: 122000, Loss: 0.46841775050399803\n",
            "Epoch: 0, Instance: 123000, Loss: 0.5270418751136949\n",
            "Epoch: 0, Instance: 124000, Loss: 0.5451990902912653\n",
            "Epoch: 0, Instance: 125000, Loss: 0.4302984199229727\n",
            "Epoch: 0, Instance: 126000, Loss: 0.355249788200204\n",
            "Epoch: 0, Instance: 127000, Loss: 0.42572803224757666\n",
            "Epoch: 0, Instance: 128000, Loss: 0.650723563594825\n",
            "Epoch: 0, Instance: 129000, Loss: 0.5217754100231695\n",
            "Epoch: 0, Instance: 130000, Loss: 0.5495231441094397\n",
            "Epoch: 0, Instance: 131000, Loss: 0.48683704100791714\n",
            "Epoch: 0, Instance: 132000, Loss: 0.5065139829030724\n",
            "Epoch: 0, Instance: 133000, Loss: 0.5686892896230085\n",
            "Epoch: 0, Instance: 134000, Loss: 0.5135319865153235\n",
            "Epoch: 0, Instance: 135000, Loss: 0.5302647775769911\n",
            "Epoch: 0, Instance: 136000, Loss: 0.5735048051607442\n",
            "Epoch: 0, Instance: 137000, Loss: 0.4320971844851001\n",
            "Epoch: 0, Instance: 138000, Loss: 0.39147313783976984\n",
            "Epoch: 0, Instance: 139000, Loss: 0.4349810196752276\n",
            "Epoch: 0, Instance: 140000, Loss: 0.5129653370843593\n",
            "Epoch: 0, Instance: 141000, Loss: 0.7834940255380164\n",
            "Epoch: 0, Instance: 142000, Loss: 0.5851922620293912\n",
            "Epoch: 0, Instance: 143000, Loss: 0.5672965095980149\n",
            "Epoch: 0, Instance: 144000, Loss: 0.5344452273526998\n",
            "Epoch: 0, Instance: 145000, Loss: 0.731810909213772\n",
            "Epoch: 0, Instance: 146000, Loss: 0.6665378680510097\n",
            "Epoch: 0, Instance: 147000, Loss: 0.6751801764216003\n",
            "Epoch: 0, Instance: 148000, Loss: 0.5807941317340531\n",
            "Epoch: 0, Instance: 149000, Loss: 0.5911935549868672\n",
            "Epoch: 0, Instance: 150000, Loss: 0.9231494250834692\n",
            "Epoch: 0, Instance: 151000, Loss: 0.45776770663103766\n",
            "Epoch: 0, Instance: 152000, Loss: 0.6694980597429099\n",
            "Epoch: 0, Instance: 153000, Loss: 0.5706771545026859\n",
            "Epoch: 0, Instance: 154000, Loss: 0.44085441425580013\n",
            "Epoch: 0, Instance: 155000, Loss: 0.5353822985453253\n",
            "Epoch: 0, Instance: 156000, Loss: 0.4463724001512669\n",
            "Epoch: 0, Instance: 157000, Loss: 0.5402059491334451\n",
            "Epoch: 0, Instance: 158000, Loss: 0.547942196388203\n",
            "Epoch: 0, Instance: 159000, Loss: 0.5867746490933958\n",
            "Epoch: 0, Instance: 160000, Loss: 0.4614563402316802\n",
            "Epoch: 0, Instance: 161000, Loss: 0.5840500638636453\n",
            "Epoch: 0, Instance: 162000, Loss: 0.6634100347981771\n",
            "Epoch: 0, Instance: 163000, Loss: 0.5140530561455252\n",
            "Epoch: 0, Instance: 164000, Loss: 0.4908256124430575\n",
            "Epoch: 0, Instance: 165000, Loss: 0.5770906244175016\n",
            "Epoch: 0, Instance: 166000, Loss: 0.6265649404294322\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     <<UNK>>       1.00      1.00      1.00       325\n",
            "         ADJ       0.81      0.71      0.75      1622\n",
            "         ADP       0.90      0.90      0.90      2481\n",
            "         ADV       0.74      0.79      0.76      1114\n",
            "         AUX       0.94      0.93      0.93      1189\n",
            "       CCONJ       0.99      0.97      0.98       839\n",
            "         DET       0.96      0.93      0.94      2111\n",
            "        INTJ       0.93      0.60      0.73       163\n",
            "        NOUN       0.83      0.88      0.85      4239\n",
            "         NUM       0.71      0.78      0.74       440\n",
            "        PART       0.85      0.87      0.86       519\n",
            "        PRON       0.90      0.95      0.92      1746\n",
            "       PROPN       0.81      0.58      0.68      1628\n",
            "       PUNCT       0.99      1.00      0.99      3027\n",
            "       SCONJ       0.54      0.60      0.57       340\n",
            "         SYM       0.62      0.57      0.60        35\n",
            "        VERB       0.78      0.86      0.82      2480\n",
            "           X       0.14      0.25      0.18        32\n",
            "\n",
            "    accuracy                           0.87     24330\n",
            "   macro avg       0.80      0.79      0.79     24330\n",
            "weighted avg       0.87      0.87      0.86     24330\n",
            "\n",
            "Embeddings Tagger Performance:\n",
            "Precision: 0.8021, Recall: 0.7861, F1-Score: 0.7895\n"
          ]
        }
      ],
      "source": [
        "precision_embeddings, recall_embeddings, f1_embeddings = train_evaluate_tagger(embeddings_tagger)\n",
        "\n",
        "print(\"Embeddings Tagger Performance:\")\n",
        "print(f\"Precision: {precision_embeddings:.4f}, Recall: {recall_embeddings:.4f}, F1-Score: {f1_embeddings:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZO0uGL-4S-"
      },
      "source": [
        "\n",
        "# Part 4 - NLTK Tagger\n",
        "\n",
        "### Overview\n",
        "In this final part of the assignment, you will evaluate the performance of the HMM-based and feed-forward taggers you developed against a Maximum Entropy Markov Model (MEMM) tagger implemented using the Natural Language Toolkit (NLTK), a popular NLP library. Perform comparison should cover the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl3oahPVpsBt"
      },
      "source": [
        "#### Step 1: Training the MEMM Tagger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CmkNPYzOvDdf"
      },
      "outputs": [],
      "source": [
        "# Create a TnT object\n",
        "tnt_tagger = tnt.TnT()\n",
        "\n",
        "# Train the TnT tagger on the training data\n",
        "tnt_tagger.train(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ8YwpiyvJ6l"
      },
      "source": [
        "#### Step 2: Evaluation\n",
        "- Evaluate the trained MEMM tagger on  the test dataset.\n",
        "- Calculate performance metrics such as accuracy, and F1-score. NLTK provides utilities that can help compute these metrics efficiently.\n",
        "- Save & Print the eveluation scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cnw1UbMj1n3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/78/hsq9362565qck24p_twrvs0cx236x4/T/ipykernel_82917/4037242059.py:2: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy_tnt_pos_tagger = tnt_tagger.evaluate(test_dataset)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8621454993834772"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accurcy\n",
        "accuracy_tnt_pos_tagger = tnt_tagger.evaluate(test_dataset)\n",
        "accuracy_tnt_pos_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3-7FX8tW0y7k"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.972972972972973"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# F-measure for each tag\n",
        "tag_predictions = [tnt_tagger.tag([word for word, _ in sentence]) for sentence in test_dataset]\n",
        "\n",
        "true_tags_tnt = [tag for sentence in test_dataset for _, tag in sentence]\n",
        "predicted_tags_tnt = [tag for sentence in tag_predictions for _, tag in sentence]\n",
        "\n",
        "f_measure_tnt_pos_tagger = f_measure(set(true_tags_tnt), set(predicted_tags_tnt))\n",
        "f_measure_tnt_pos_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TnT Tagger Evaluation:\n",
            "  Precision: 0.88\n",
            "  Recall: 0.78\n",
            "  F1-score: 0.82\n",
            "  Accuracy: 0.86\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fouadaz/toolsInstallation/anaconda3/envs/myenvtagging/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import json\n",
        "\n",
        "\n",
        "# Step 3: Calculate Performance Metrics\n",
        "def compute_metrics(true_tags, pred_tags):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_tags, pred_tags, average='macro')\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "def print_evaluation_scores(name, precision, recall, f1, accuracy):\n",
        "    print(f\"{name} Evaluation:\")\n",
        "    print(f\"  Precision: {precision:.2f}\")\n",
        "    print(f\"  Recall: {recall:.2f}\")\n",
        "    print(f\"  F1-score: {f1:.2f}\")\n",
        "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Calculate performance metrics\n",
        "precision_tnt, recall_tnt, f1_tnt, accuracy_tnt = compute_metrics(true_tags_tnt,predicted_tags_tnt)\n",
        "\n",
        "# Print evaluation scores\n",
        "print_evaluation_scores(\"TnT Tagger\", precision_tnt, recall_tnt, f1_tnt, accuracy_tnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparison of Tagger Performance:\n",
            "TnT Tagger - Precision: 0.88, Recall: 0.78, F1-score: 0.82\n",
            "HMM Tagger - Precision: 0.79, Recall: 0.74, F1-score: 0.75\n",
            "Feed-forward Neural Network Tagger - Precision: 0.80, Recall: 0.79, F1-score: 0.79\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print and compare the results\n",
        "print(\"\\nComparison of Tagger Performance:\")\n",
        "print(f\"TnT Tagger - Precision: {precision_tnt:.2f}, Recall: {recall_tnt:.2f}, F1-score: {f1_tnt:.2f}\")\n",
        "print(f\"HMM Tagger - Precision: {precision_hmm:.2f}, Recall: {recall_hmm:.2f}, F1-score: {f1_hmm:.2f}\")\n",
        "print(f\"Feed-forward Neural Network Tagger - Precision: {precision_embeddings:.2f}, Recall: {recall_embeddings:.2f}, F1-score: {f1_embeddings:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG4PmdJ_qRHi"
      },
      "source": [
        "# Evaluatoin and Comparison\n",
        "Compare the results obtained from the MEMM tagger with those from your HMM and feed-forward neural network taggers.\n",
        "\n",
        "This part won't be tested by the autograder.\n",
        "\n",
        "#### Discuss the following:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXpQoDsdqihY"
      },
      "source": [
        "\n",
        "* <font color='red'>(**?**)</font> Which tagger performed best on each dataset and why?\n",
        "* <font color='green'>(**Answer**)</font> :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #06768d\">\n",
        "\n",
        "The $\\textbf{TnT tagger (MEMM)}$ performed the best overall, with a precision of 0.88 and an F1-score of 0.82. Its high precision means it makes very accurate predictions with fewer false positives. This is largely due to its ability to handle rich contextual information and overlapping features.\n",
        "\n",
        "The $\\textbf{feed-forward neural network tagger}$ also performed well, with a precision of 0.80, recall of 0.79, and F1-score of 0.79. This tagger benefits from using pre-trained word embeddings trained by experts on rich datasets, which provide a strong understanding of word semantics. Combined with a linear layer that connects the word embedding of current and previous words, and the previous tag (represented as a one hot vector) to the tag predictions allows to model complex patterns and relationships.\n",
        "\n",
        "The $\\textbf{HMM tagger}$ had the weakest performance, with a precision of 0.79, recall of 0.74, and F1-score of 0.75. While it provides a solid baseline, its simpler sequential modeling approach struggles with capturing complex dependencies and longer-range contextual features compared to the other two taggers.\n",
        "\n",
        "In summary, the $\\textbf{TnT tagger}$ is the top performer due to its ability to effectively use contextual information. The $\\textbf{feed-forward neural network tagger}$ also shows strong performance thanks to pre-trained embeddings and complex pattern recognition. The $\\textbf{HMM tagger}$, although decent, falls short in handling more intricate dependencies and contextual information.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SgZs91PqyIG"
      },
      "source": [
        "* <font color='red'>(**?**)</font> What are the strengths and weaknesses of each approach (HMM, feed-forward neural network, MEMM) in terms of training time, accuracy, and generalizability?\n",
        "* <font color='green'>(**Answer**)</font> :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #06768d\">\n",
        "\n",
        "\n",
        "$\\textbf{HMM tagger}$ are quick and easy to use, but they fall short in accuracy and handling complex patterns. This is because HMMs rely on simple probability estimations, which may not capture long-range dependencies or intricate contextual information effectively. They are fast to train but have limited generalizability.\n",
        "\n",
        "$\\textbf{Feed-forward neural network tagger}$ offer excellent accuracy and generalizability. They can model complex, non-linear relationships in the data and use pre-trained word embeddings to enhance semantic understanding. However, this high accuracy and generalizability come at the cost of longer training times and higher computational demands.\n",
        "\n",
        "$\\textbf{TnT tagger}$ (MEMMs) strike a balance by providing high accuracy and effective use of context. They leverage rich contextual information and overlapping features to make precise predictions. Nonetheless, they require more time to train and risk overfitting if not properly regularized. While they offer high accuracy, their generalizability can be affected by overfitting issues.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9_TVEPUKkO"
      },
      "source": [
        "# Testing\n",
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "mJgUy9qNUE0l",
        "outputId": "2edd423e-b827-4a4a-e1c7-2c99518283e4"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "def test_read_data_data_types():\n",
        "    data = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n",
        "    result = {\n",
        "        'is_data_list': type(data) == list,\n",
        "        'is_data_first_element_list': type(data[0]) == list,\n",
        "        'is_data_first_element_first_item_tuple': type(data[0][0]) == tuple\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def test_read_data_len_train_data():\n",
        "    return {\n",
        "        'train_data_length': len(read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")),\n",
        "    }\n",
        "\n",
        "def test_generate_vocabs():\n",
        "    return {\n",
        "        'vocab_size': len(words_vocab),\n",
        "        'num_tags': len(tags_vocab)\n",
        "    }\n",
        "\n",
        "def test_hmm():\n",
        "    return {\n",
        "        'precision': round(precision_hmm, 2),\n",
        "        'recall': round(recall_hmm, 2),\n",
        "        'f1': round(f1_hmm, 2),\n",
        "    }\n",
        "\n",
        "def test_embeddings_model():\n",
        "    return {\n",
        "        'precision': round(precision_embeddings, 2),\n",
        "        'recall': round(recall_embeddings, 2),\n",
        "        'f1': round(f1_embeddings, 2),\n",
        "    }\n",
        "    \n",
        "\n",
        "def test_nltk_tagger():\n",
        "    return {\n",
        "        'accuracy': round(accuracy_tnt_pos_tagger, 2)\n",
        "    }\n",
        "\n",
        "TESTS = [test_read_data_data_types, test_read_data_len_train_data, test_generate_vocabs, test_hmm, test_embeddings_model, test_nltk_tagger]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "# files.download('results.json')\n",
        "\n",
        "####################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
